import os
import pickle
import time
import re
import unicodedata
import requests
from bs4 import BeautifulSoup
from langchain.schema import Document
# ë¬¸ì„œ ë‹¨ìœ„ ìœ ì§€ (ë¶„í•  X)
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_chroma import Chroma
import fitz  # PyMuPDF
from PIL import Image
import pytesseract
from dataclasses import dataclass
from dotenv import load_dotenv
from typing import List, Dict, Any, Tuple
from difflib import SequenceMatcher
from urllib.parse import urlparse, parse_qs

# =========================
# Config & Globals
# =========================

@dataclass
class WebSourceConfig:
    cache_file: str
    base_url: str
    base_list_url: str
    source_type: str
    num_pages: int
    menu_no: int

PDF_FOLDER = "KB_web_docs"
PDF_CACHE_FILE = "pdf_cache.pkl"
PROCESSED_URLS_FILE = "processed_urls.pkl"

VECTOR_STORE_DIR = "vector_store"                 # ë³¸ë¬¸(ë¬¸ì„œ) ë²¡í„°
TITLE_VECTOR_STORE_DIR = "vector_store_titles"    # ì œëª© ì „ìš© ë²¡í„°

WEB_CACHE_FILE = "web_cache.pkl"
FAQS_CACHE_FILE = "faqs_cache.pkl"

load_dotenv()
_api_key = os.getenv("GOOGLE_API_KEY")
if not isinstance(_api_key, str) or not _api_key.strip():
    raise ValueError("GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ì— ìœ íš¨í•œ í‚¤ë¥¼ ë„£ì–´ì£¼ì„¸ìš”.")
os.environ["GOOGLE_API_KEY"] = _api_key.strip()

# ê³µìš© ì„ë² ë”©/LLM (ì¬ì‚¬ìš©)
EMBEDDINGS = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
LLM = ChatGoogleGenerativeAI(model="gemini-2.0-flash", temperature=0)

# =========================
# í•œêµ­ì–´ ì§ˆì˜ ì „ì²˜ë¦¬/í™•ì¥
# =========================

_JOSA = r"(ì€|ëŠ”|ì´|ê°€|ì„|ë¥¼|ì—|ì—ì„œ|ìœ¼ë¡œ|ì™€|ê³¼|ë„|ë§Œ|ë¶€í„°|ê¹Œì§€|ë§ˆë‹¤|ì²˜ëŸ¼|ë³´ë‹¤|ì˜|ë¡œì„œ|ì´ë¼ë©´|ì´ë¼ë„|ì´ë©°|ì´ê³ )$"
_EOMI = r"(í•˜ë‹¤|í•©ë‹ˆë‹¤|í•´ìš”|í–ˆë‚˜ìš”|ë˜ë‚˜ìš”|ë˜ì—ˆë‚˜ìš”|ë©ë‹ˆê¹Œ|ì¸ê°€ìš”|ì¸ê°€ìš”\?|ì¸ê°€|ì¼ê¹Œìš”|ì¼ê¹Œ|ì˜€ë‹¤|ì˜€ë‹¤ê°€|ë˜ì—ˆë‹¤|í•œë‹¤)$"

def _strip_accents(text: str) -> str:
    return unicodedata.normalize("NFKC", text)

def _basic_cleanup(text: str) -> str:
    text = _strip_accents(text).lower()
    text = text.replace("ï¼…", "%").replace("â€¢", " ").replace("Â·", " ")
    text = re.sub(r"[â€œâ€\"'`]", " ", text)
    text = re.sub(r"[()\[\]{}]", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text

def _light_morph_simplify_token(tok: str) -> str:
    tok = re.sub(_JOSA, "", tok)
    tok = re.sub(_EOMI, "í•˜ë‹¤", tok)
    return tok

def _normalize_korean_query(text: str) -> str:
    t = _basic_cleanup(text)
    t = re.sub(r"(\d+)\s*í¼ì„¼íŠ¸", r"\1%", t)
    t = re.sub(r"(\d+)\s*í¼\s*ì„¼\s*íŠ¸", r"\1%", t)
    t = t.replace("%p", "%")
    tokens = []
    for tok in t.split(" "):
        if not tok:
            continue
        if re.search(r"[ê°€-í£]", tok):
            tok = _light_morph_simplify_token(tok)
        tokens.append(tok)
    t = " ".join(tokens)
    t = re.sub(r"\s+", " ", t).strip()
    return t

def generate_query_variants_korean(text: str, max_variants: int = 8) -> List[str]:
    base_raw = _basic_cleanup(text)
    stripped = re.sub(r"(ê´€ë ¨|ê´€í•œ|ê´€ë ¨ëœ)\b", " ", base_raw)
    stripped = re.sub(r"(q\s*&\s*a|q\/a|q&a|qa|ì§ˆë¬¸\s*ë‹µë³€|ì§ˆë‹µ)", " ", stripped)
    stripped = re.sub(r"\s+", " ", stripped).strip()
    base = _normalize_korean_query(text)
    variants = [base]
    if stripped and stripped != base_raw:
        variants.append(_normalize_korean_query(stripped))
    compact = re.sub(r"\s+", "", base)
    if compact != base:
        variants.append(compact)
    no_punct = re.sub(r"[-/]", " ", base)
    no_punct = re.sub(r"\s+", " ", no_punct).strip()
    if no_punct not in variants:
        variants.append(no_punct)
    attach_num_unit = re.sub(r"(\d+)\s*%", r"\1%", base)
    if attach_num_unit not in variants:
        variants.append(attach_num_unit)
    uniq = []
    for v in variants:
        if v and v not in uniq:
            uniq.append(v)
    return uniq[:max_variants]

# =========================
# Web scraping
# =========================

def get_table_links(page_index, menu_no, base_list_url, base_url):
    params = {"menuNo": menu_no, "pageIndex": page_index}
    resp = requests.get(base_list_url, params=params, timeout=15)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, "html.parser")
    links = []
    for a_tag in soup.select("a[href*='view.do']"):
        href = a_tag.get("href")
        if not href:
            continue
        if href.startswith("."):
            href = href.lstrip("./")  # ì•ˆì „í•˜ê²Œ ë³´ì •
        links.append(base_url + href)
    return links

def get_web_page_details(url, source_type):
    resp = requests.get(url, timeout=15)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, "html.parser")
    subject_tag = soup.find("h2", class_="subject")
    title = subject_tag.get_text(strip=True) if subject_tag else "No subject found"
    content_tag = soup.find("div", class_="dbdata")
    content_text = (
        content_tag.get_text(separator="\n", strip=True)
        if content_tag
        else "No content found"
    )
    print(f"Fetched case: {title}")
    print(content_text[:100])
    return Document(
        page_content=content_text,
        metadata={"source": url, "title": title, "source_type": source_type},
    )

def scrape_web_docs(web_source_confing: WebSourceConfig, force_refresh=False):
    if not force_refresh and os.path.exists(web_source_confing.cache_file):
        print(f"Loading {web_source_confing.source_type} docs from cache...")
        with open(web_source_confing.cache_file, "rb") as f:
            return pickle.load(f)

    print(f"Scraping {web_source_confing.source_type} docs from website...")
    if os.path.exists(PROCESSED_URLS_FILE):
        with open(PROCESSED_URLS_FILE, "rb") as f:
            processed_urls = pickle.load(f)
    else:
        processed_urls = set()

    all_docs: List[Document] = []
    for page in range(1, web_source_confing.num_pages + 1):
        print(f"Fetching list page {page}...")
        case_links = get_table_links(
            page,
            web_source_confing.menu_no,
            web_source_confing.base_list_url,
            web_source_confing.base_url,
        )
        for link in case_links:
            if link in processed_urls:
                continue
            print(f"  Fetching case: {link}")
            try:
                doc = get_web_page_details(link, web_source_confing.source_type)
                all_docs.append(doc)
                processed_urls.add(link)
                time.sleep(0.2)
            except Exception as e:
                print(f"    Failed to fetch {link}: {e}")

    with open(web_source_confing.cache_file, "wb") as f:
        pickle.dump(all_docs, f)
    with open(PROCESSED_URLS_FILE, "wb") as f:
        pickle.dump(processed_urls, f)
    print(f"Saved {len(all_docs)} {web_source_confing.source_type} docs to cache.")
    return all_docs

# =========================
# PDF loading with caching and OCR fallback
# =========================

def load_pdf_with_ocr(path):
    text = ""
    doc = fitz.open(path)
    for page in doc:
        pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        text += pytesseract.image_to_string(img, lang="kor+eng")
    return text

def load_pdfs(folder, force_refresh=False):
    if not force_refresh and os.path.exists(PDF_CACHE_FILE):
        print("Loading PDF docs from cache...")
        with open(PDF_CACHE_FILE, "rb") as f:
            return pickle.load(f)

    print("Loading PDFs from folder...")
    docs: List[Document] = []
    if not os.path.isdir(folder):
        print(f"  PDF í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {folder}")
        with open(PDF_CACHE_FILE, "wb") as f:
            pickle.dump(docs, f)
        return docs

    for file in os.listdir(folder):
        if not file.lower().endswith(".pdf"):
            continue
        print(f"Processing {file}...")
        file_path = os.path.join(folder, file)
        try:
            from langchain_community.document_loaders import PyPDFLoader
            loader = PyPDFLoader(file_path)
            docs.extend(loader.load())
        except Exception:
            print(f"  Fallback to OCR for {file}")
            try:
                text = load_pdf_with_ocr(file_path)
                docs.append(
                    Document(
                        page_content=text,
                        metadata={"source": os.path.join(folder, file), "source_type": "PDF_OCR"},
                    )
                )
            except Exception as e2:
                print(f"  OCR ì‹¤íŒ¨: {file} ({e2})")
    with open(PDF_CACHE_FILE, "wb") as f:
        pickle.dump(docs, f)
    print(f"Saved {len(docs)} PDF-page docs to cache.")
    return docs

def combine_pdf_pages_by_file(page_docs: List[Document]) -> List[Document]:
    by_file: Dict[str, List[str]] = {}
    for d in page_docs:
        src = d.metadata.get("source", "unknown.pdf")
        by_file.setdefault(src, []).append(d.page_content)

    combined: List[Document] = []
    for src, pages in by_file.items():
        full_text = "\n\n".join(pages)
        source_type = "PDF_OCR" if any("PDF_OCR" == d.metadata.get("source_type") for d in page_docs if d.metadata.get("source") == src) else "PDF"
        combined.append(
            Document(
                page_content=full_text,
                metadata={"source": src, "title": os.path.basename(src), "source_type": source_type},
            )
        )
    print(f"Combined into {len(combined)} PDF-file docs.")
    return combined

# =========================
# Build or update vector store (ë¬¸ì„œ ë‹¨ìœ„ + ì œëª© ì¸ë±ìŠ¤)
# =========================

def _ensure_chroma(dir_path: str, docs: List[Document]) -> Chroma:
    if os.path.exists(dir_path):
        db = Chroma(persist_directory=dir_path, embedding_function=EMBEDDINGS)
        if docs:
            db.add_documents(docs)
            try:
                db.persist()
            except Exception:
                pass
        return db
    else:
        db = Chroma.from_documents(docs, EMBEDDINGS, persist_directory=dir_path)
        try:
            db.persist()
        except Exception:
            pass
        return db

def build_or_update_vectorstore(force_refresh: bool = False):
    pdf_page_docs = load_pdfs(PDF_FOLDER, force_refresh=force_refresh)
    pdf_docs = combine_pdf_pages_by_file(pdf_page_docs)

    disputes_config = WebSourceConfig(
        cache_file=WEB_CACHE_FILE,
        base_url="https://www.fss.or.kr/fss/job/fvsttPrcdnt",
        base_list_url="https://www.fss.or.kr/fss/job/fvsttPrcdnt/list.do",
        source_type="DISPUTES",
        num_pages=56,
        menu_no=200179,
    )
    faqs_config = WebSourceConfig(
        cache_file=FAQS_CACHE_FILE,
        base_url="https://www.fss.or.kr",
        base_list_url="https://www.fss.or.kr/fss/bbs/B0000172/list.do",
        source_type="FAQS",
        num_pages=94,
        menu_no=200202,
    )
    dispute_docs = scrape_web_docs(disputes_config, force_refresh=force_refresh)
    faq_docs = scrape_web_docs(faqs_config, force_refresh=force_refresh)

    all_docs = pdf_docs + dispute_docs + faq_docs

    title_docs = []
    for d in all_docs:
        title = (d.metadata or {}).get("title") or (d.metadata or {}).get("source") or ""
        title_norm = _basic_cleanup(title)
        title_docs.append(Document(page_content=title_norm, metadata=d.metadata))

    body_db = _ensure_chroma(VECTOR_STORE_DIR, all_docs)
    title_db = _ensure_chroma(TITLE_VECTOR_STORE_DIR, title_docs)
    return body_db, title_db

# =========================
# Scoring helpers
# =========================

def _to_similarity(dist: float, metric: str = "cosine") -> float:
    if metric == "cosine":
        return 1.0 - float(dist)  # cosine_distance = 1 - cosine_similarity
    elif metric in ("l2", "euclidean"):
        return 1.0 / (1.0 + float(dist))
    else:
        return 1.0 / (1.0 + float(dist))

def _zscore(xs: List[float]) -> List[float]:
    if not xs:
        return xs
    m = sum(xs) / len(xs)
    v = sum((x - m) ** 2 for x in xs) / max(1, len(xs)-1)
    sd = v ** 0.5
    if sd < 1e-12:
        return [0.0 for _ in xs]
    return [(x - m) / sd for x in xs]

def _softmax(xs: List[float], tau: float = 1.0) -> List[float]:
    import math
    if not xs:
        return xs
    mx = max(xs)
    exps = [math.exp((x - mx) / max(1e-8, tau)) for x in xs]
    s = sum(exps)
    return [e / max(1e-12, s) for e in exps]

def _aggregate_variants_to_best(sim_list: List[float], mode: str = "softmax-mean") -> float:
    if not sim_list:
        return 0.0
    if mode == "max":
        return max(sim_list)
    if mode == "mean":
        return sum(sim_list) / len(sim_list)
    if mode == "softmax-mean":
        w = _softmax(sim_list, tau=0.25)
        return sum(si * wi for si, wi in zip(sim_list, w))
    return max(sim_list)

def _apply_priors(meta: Dict[str, Any]) -> float:
    st = (meta or {}).get("source_type", "")
    prior = 0.0
    if st in ("FAQS", "FAQ"):
        prior += 0.03
    elif st == "PDF":
        prior += 0.01
    return prior

def _fuse_title_body_scores(
    title_pairs: List[Tuple[Document, float]],
    body_map: Dict[str, float],
    w_title: float = 0.6,
    w_body: float = 0.4,
    use_rrf: bool = False,
) -> List[Tuple[Document, float]]:
    titles = [s for (_, s) in title_pairs]
    bodies = [body_map.get((d.metadata or {}).get("source", ""), 0.0) for (d, _) in title_pairs]

    if use_rrf:
        def _ranks(vals):
            order = sorted(range(len(vals)), key=lambda i: vals[i], reverse=True)
            rank = [0]*len(vals)
            for r, i in enumerate(order):
                rank[i] = r + 1
            return rank
        k = 60
        rt = _ranks(titles)
        rb = _ranks(bodies)
        fused = []
        for (doc, _), rti, rbi in zip(title_pairs, rt, rb):
            score = (1.0/(k + rti)) + (1.0/(k + rbi)) + _apply_priors(doc.metadata)
            fused.append((doc, score))
        fused.sort(key=lambda x: x[1], reverse=True)
        return fused

    zt = _zscore(titles)
    zb = _zscore(bodies)
    fused = []
    for (doc, _), ts, bs in zip(title_pairs, zt, zb):
        score = w_title*ts + w_body*bs + _apply_priors(doc.metadata)
        fused.append((doc, score))
    fused.sort(key=lambda x: x[1], reverse=True)
    return fused

# =========================
# ğŸ”§ ë™ì  íƒ€ì´ë¸Œë ˆì´ì»¤ + Lexical ê²Œì´íŠ¸
# =========================
# PATCH-3: ë¶€ë¶„ í† í°(n-gram) í™•ì¥ ì§€ì›

def _tokenize_simple(s: str) -> List[str]:
    """
    ê³µë°± ê¸°ì¤€ 1ì°¨ í† í°í™” í›„, ê° í† í°ì— ëŒ€í•´ 2~3ê¸€ì n-gram ì„œë¸Œí† í°ì„ ì¶”ê°€í•œë‹¤.
    - í•œêµ­ì–´/ì˜ë¬¸/ìˆ«ìë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ëŠ” ì œê±°
    - ë„ˆë¬´ ì§§ì€ ì¡°ê° í­ì¦ ë°©ì§€ ìœ„í•´ ê¸¸ì´ ì œí•œ ë° dedup ì ìš©
    """
    s = _basic_cleanup(s)
    base = [t for t in s.split() if t]
    cleaned = []
    for t in base:
        t = re.sub(r"[^0-9a-zê°€-í£]", "", t)  # ë¬¸ì/ìˆ«ìë§Œ
        if t:
            cleaned.append(t)

    out: set = set()
    for t in cleaned:
        out.add(t)
        L = len(t)
        # 2-gram
        if L >= 2:
            for i in range(L - 1):
                out.add(t[i:i+2])
        # 3-gram (ê¸¸ì´ 6 ì´ìƒì¼ ë•Œë§Œ ì¶”ê°€ë¡œ ìƒì„±í•´ì„œ í­ì¦ ë°©ì§€)
        if L >= 6:
            for i in range(L - 2):
                out.add(t[i:i+3])

    # ë„ˆë¬´ ì§§ì€ ë‹¨í¸ì€ ì œê±°(1ê¸€ì ì œì™¸) â€” í•œêµ­ì–´/ì˜ë¬¸ í˜¼ìš© ëŒ€ë¹„
    out = {tok for tok in out if len(tok) >= 2}
    return list(out)

def _jaccard(a: List[str], b: List[str]) -> float:
    if not a or not b:
        return 0.0
    A, B = set(a), set(b)
    inter = len(A & B)
    union = len(A | B)
    return inter / union if union else 0.0

def _extract_recency_from_url(url: str) -> int:
    try:
        qs = parse_qs(urlparse(url).query)
        for key in ("nttId", "incdnSlno"):
            if key in qs and qs[key]:
                return int(qs[key][0])
    except Exception:
        pass
    try:
        qs = parse_qs(urlparse(url).query)
        pi = int(qs.get("pageIndex", [0])[0])
        return max(1, 100000 - pi)
    except Exception:
        return 0

def _source_type_weight(meta: Dict[str, Any]) -> float:
    st = (meta or {}).get("source_type", "")
    return {"FAQS": 3.0, "FAQ": 3.0, "DISPUTES": 2.0, "PDF": 1.0}.get(st, 0.0)

def _title_tiebreak_key(doc: Document, sim: float, query: str) -> tuple:
    title = (doc.metadata or {}).get("title") or ""
    url   = (doc.metadata or {}).get("source") or ""
    t_norm = _basic_cleanup(title)
    q_norm = _basic_cleanup(query)
    toks_t = _tokenize_simple(title)
    toks_q = _tokenize_simple(query)
    exact = 1 if t_norm == q_norm else 0
    starts = 1 if (t_norm.startswith(q_norm) or q_norm.startswith(t_norm)) else 0
    contains = 1 if (q_norm in t_norm or t_norm in q_norm) else 0
    jac = _jaccard(toks_t, toks_q)
    ratio = SequenceMatcher(None, q_norm, t_norm).ratio()
    st_w = _source_type_weight(doc.metadata)
    rec = _extract_recency_from_url(url)
    short_bonus = -len(t_norm)
    return (sim, exact, starts, contains, jac, ratio, st_w, rec, short_bonus)

# === íŒ¨ì¹˜ 1: í•µì‹¬ í† í° ê¸°ë°˜ ë¶ˆìš©ì–´ ì œê±° + ê°•í™”ëœ ê²Œì´íŠ¸ ===
STOPWORDS_KO = {
    "ê´€ë ¨","ê´€í•œ","ê´€ë ¨ëœ","ì§ˆë¬¸","ë‹µë³€","q&a","qa","q/a",
    "ì–´ë–»ê²Œ","ë˜ë‚˜ìš”","ë˜ë‚˜","ë©ë‹ˆê¹Œ","ë¬´ì—‡","ë¬´ì—‡ì¸ê°€ìš”",
    "ê°€ëŠ¥","ì¸ê°€ìš”","ì´ë‹¤","í•˜ë‹¤","í•©ë‹ˆë‹¤","í•´ìš”",
    "ì•ˆë‚´","ìœ ì˜ì‚¬í•­","ë¬¸ì˜","ë°”ë¡œê°€ê¸°"
}

def _content_tokens(s: str) -> List[str]:
    toks = _tokenize_simple(s)
    return [t for t in toks if t not in STOPWORDS_KO and len(t) >= 2]

def _passes_lexical_gate(title: str, query: str, min_jaccard: float = 0.08) -> bool:
    toks_t = _content_tokens(title)
    toks_q = _content_tokens(query)
    if not toks_t or not toks_q:
        return False
    overlap = len(set(toks_t) & set(toks_q))
    need = 1 if len(toks_q) <= 3 else 2  # ìµœì†Œ ê²¹ì¹¨ ê°œìˆ˜
    if overlap < need:
        return False
    jac = _jaccard(toks_t, toks_q)
    t_norm = _basic_cleanup(title)
    q_norm = _basic_cleanup(query)
    contains = (q_norm in t_norm) or (t_norm in q_norm)
    return (jac >= min_jaccard) or contains

# =========================
# ë¬¸ì„œ ë‚´ë¶€ ë¡œì»¬ ê´€ë ¨ë„ (ì¸ë±ìŠ¤ëŠ” ë¬¸ì„œ ë‹¨ìœ„ ìœ ì§€)
# =========================

STOP_PHRASES = {"ê³ ê°ì„¼í„°","ìœ ì˜ì‚¬í•­","ë¬¸ì˜","ìë£Œì‹¤","ë°”ë¡œê°€ê¸°"}

def split_passages(text: str, target_len: int = 220, stride: int = 110) -> List[str]:
    """
    ì¸ë±ìŠ¤ëŠ” ë¬¸ì„œë‹¨ìœ„ ìœ ì§€. ëŸ°íƒ€ì„ì—ë§Œ ë¬¸ì„œ ë‚´ë¶€ë¥¼ ì–•ê²Œ ìŠ¬ë¼ì´ìŠ¤.
    í† í°ì€ _tokenize_simple ì¬ì‚¬ìš©(ë¶€ë¶„ n-gram í¬í•¨).
    """
    toks = _tokenize_simple(text)
    if not toks:
        return []
    passages = []
    for i in range(0, max(1, len(toks)-target_len+1), stride):
        chunk = " ".join(toks[i:i+target_len])
        if chunk:
            passages.append(chunk)
        if len(passages) >= 80:  # ì•ˆì „ ìƒí•œ
            break
    if not passages:
        passages = [" ".join(toks[:target_len])]
    return passages

def keyword_overlap(q_tokens: List[str], p_tokens: List[str]) -> float:
    Q, P = set(q_tokens), set(p_tokens)
    inter = len(Q & P)
    return inter / max(1, len(Q))

def local_score_simple(query: str, passage: str) -> float:
    tq = _tokenize_simple(query)
    tp = _tokenize_simple(passage)
    if not tq or not tp:
        return 0.0
    jac = _jaccard(tq, tp)
    cover = keyword_overlap(tq, tp)
    return 0.5 * jac + 0.5 * cover

def boiler_penalty(text: str) -> float:
    t = _basic_cleanup(text)
    hits = sum(1 for s in STOP_PHRASES if s in t)
    return 0.02 * hits  # ìµœì¢… ì ìˆ˜ì—ì„œ ëº„ ì˜ˆì •

def doc_keyword_coverage(query: str, text: str) -> float:
    tq = set(_tokenize_simple(query))
    tt = set(_tokenize_simple(text))
    if not tq or not tt:
        return 0.0
    return len(tq & tt) / max(1, len(tq))

def rerank_inside_doc(doc: Document, query: str, emb_sim: float) -> Tuple[float, Dict[str, float]]:
    """
    ìµœì¢… ì ìˆ˜ = Î±*ì„ë² ë”©ìœ ì‚¬ë„ + Î²*ë¬¸ì„œë‚´ë¡œì»¬ìµœëŒ€ + Î³*ì§ˆì˜ì»¤ë²„ë¦¬ì§€ + Î´*ì œëª©ë³´ë„ˆìŠ¤ - Î»*ë³´ì¼ëŸ¬íŒ¨ë„í‹°
    """
    text = doc.page_content or ""
    passages = split_passages(text, target_len=220, stride=110)
    max_local = 0.0
    for p in passages:
        s = local_score_simple(query, p)
        if s > max_local:
            max_local = s

    cover = doc_keyword_coverage(query, text)
    title = (doc.metadata or {}).get("title", "") or ""
    title_hit = local_score_simple(query, title)
    penalty = boiler_penalty(text)

    # í•˜ë“œ ê°€ë“œ: ë¡œì»¬Â·ì»¤ë²„ ëª¨ë‘ 0ì´ë©´ íƒˆë½
    if max_local == 0.0 and cover == 0.0:
        parts = {"emb":emb_sim, "local":0.0, "cover":0.0, "title":title_hit, "pen":penalty, "final":-1e9}
        return -1e9, parts

    # ê°€ì¤‘ì¹˜
    alpha, beta, gamma, delta, lamb = 0.6, 0.25, 0.10, 0.05, 1.0
    final = alpha*emb_sim + beta*max_local + gamma*cover + delta*title_hit - lamb*penalty
    parts = {"emb":emb_sim, "local":max_local, "cover":cover, "title":title_hit, "pen":penalty, "final":final}
    return final, parts

# =========================
# 2ë‹¨ê³„ ê²€ìƒ‰ (ì œëª© ì„ í•„í„° â†’ ë³¸ë¬¸ ì¬í‰ê°€) + í•œêµ­ì–´ ì¿¼ë¦¬ í™•ì¥ + ì ì‘í˜• k + ê²Œì´íŠ¸
# =========================

def staged_retrieve(body_db: Chroma, title_db: Chroma, question: str, topk: int = 8,
                    title_k: int = 100, body_k_per_filter: int = 16,
                    metric: str = "cosine",
                    variant_agg: str = "softmax-mean",
                    use_rrf: bool = False) -> List[Document]:
    variants = generate_query_variants_korean(question)
    # print(f"[Query variants] {variants}")

    def run_title_search(k_val: int, use_gate: bool = True):
        title_best_sims: Dict[str, List[float]] = {}
        title_rep: Dict[str, Document] = {}
        for q in variants:
            res = title_db.similarity_search_with_score(q, k=k_val)
            for d, dist in res:
                if use_gate and not _passes_lexical_gate((d.metadata or {}).get("title",""), q):
                    continue
                src = (d.metadata or {}).get("source", "")
                sim = _to_similarity(dist, metric)
                if src not in title_best_sims:
                    title_best_sims[src] = []
                    title_rep[src] = d
                title_best_sims[src].append(sim)
        return title_best_sims, title_rep

    # 1ì°¨ ì‹œë„ (gate ON)
    title_best_sims, title_rep = run_title_search(title_k, use_gate=True)

    # ì‹¤íŒ¨ ì‹œ k í™•ì¥ (gate ON)
    if not title_best_sims:
        for k_try in (300, 800):
            title_best_sims, title_rep = run_title_search(k_try, use_gate=True)
            if title_best_sims:
                print(f"[title-stage] expanded k â†’ {k_try}")
                break

    # ê·¸ë˜ë„ ì‹¤íŒ¨ë©´ gate OFF fallback
    if not title_best_sims:
        title_best_sims, title_rep = run_title_search(800, use_gate=False)
        if title_best_sims:
            print("[title-stage] no gate fallback used")

    if not title_best_sims:
        return []

    title_final: Dict[str, float] = {src: _aggregate_variants_to_best(sims, mode=variant_agg)
                                     for src, sims in title_best_sims.items()}

    # Stage 2: ë³¸ë¬¸ ì¬ê²€ìƒ‰ (ì œëª© í›„ë³´ë§Œ)
    candidate_sources = list(title_final.keys())
    body_sims: Dict[str, List[float]] = {src: [] for src in candidate_sources}
    for q in variants:
        try:
            res = body_db.similarity_search_with_score(
                q, k=body_k_per_filter, filter={"source": {"$in": candidate_sources}}
            )
        except Exception:
            res = body_db.similarity_search_with_score(q, k=max(body_k_per_filter, 64))
            res = [(d, dist) for d, dist in res if (d.metadata or {}).get("source", "") in candidate_sources]
        for d, dist in res:
            src = (d.metadata or {}).get("source", "")
            body_sims[src].append(_to_similarity(dist, metric))

    body_final: Dict[str, float] = {src: _aggregate_variants_to_best(sims, mode=variant_agg)
                                    for src, sims in body_sims.items()}

    # ê²°í•© + íƒ€ì´ë¸Œë ˆì´í¬(ê¸°ì¡´)
    title_pairs = [(title_rep[src], title_final.get(src, 0.0)) for src in candidate_sources]
    merged = _fuse_title_body_scores(title_pairs, body_final, w_title=0.6, w_body=0.4, use_rrf=use_rrf)
    merged.sort(key=lambda x: _title_tiebreak_key(x[0], x[1], question), reverse=True)

    # ë¬¸ì„œ ë‚´ë¶€ ë¡œì»¬ ê´€ë ¨ë„ ê¸°ë°˜ ì¬ë­í¬(ì¸ë±ìŠ¤ëŠ” ë¬¸ì„œ ë‹¨ìœ„ ìœ ì§€)
    per_source_best: Dict[str, Tuple[Document, float, Dict[str, float]]] = {}

    for doc, _ in merged:
        src = (doc.metadata or {}).get("source", "")
        if src in per_source_best:
            continue  # source ë‹¹ í•˜ë‚˜ë§Œ í‰ê°€(ë¹„ìš© ì ˆê°)

        # ì¿¼ë¦¬ ê¸°ì¤€ìœ¼ë¡œ ê·¸ source ë‚´ì—ì„œ ê°€ì¥ ê´€ë ¨ìˆëŠ” ë³¸ë¬¸ ë¬¸ì„œë¥¼ 1ê°œ ê³ ë¦„
        try:
            q_best = body_db.similarity_search_with_score(
                question, k=3, filter={"source": src}
            )
        except Exception:
            q_best = []
        if not q_best:
            # fallback: ì œëª© ëŒ€í‘œì˜ í…ìŠ¤íŠ¸ ê¸°ì¤€ìœ¼ë¡œ íƒìƒ‰
            try:
                q_best = body_db.similarity_search_with_score(doc.page_content, k=1, filter={"source": src})
            except Exception:
                q_best = []

        if not q_best:
            continue

        bd, dist = q_best[0]
        emb_sim = _to_similarity(dist, metric)
        final_score, parts = rerank_inside_doc(bd, question, emb_sim)
        per_source_best[src] = (bd, final_score, parts)

    # ìµœì¢… ìƒìœ„ topk ì„ íƒ
    ranked = sorted(per_source_best.values(), key=lambda x: x[1], reverse=True)
    selected_docs: List[Document] = []
    for bd, score, parts in ranked[:topk]:
        # ë””ë²„ê¹… í™•ì¸ìš©
        # print(f"[final-rerank] src={ (bd.metadata or {}).get('source','') } "
        #       f"score={score:.4f} parts={parts}")
        selected_docs.append(bd)

    return selected_docs

# =========================
# Pair-by-pair cache viewer (2ê°œì”© ë³´ê¸°)
# =========================

def _print_doc_block(doc: Document, idx: int, full: bool = False, max_chars: int = 1200) -> None:
    meta = doc.metadata or {}
    title = meta.get("title") or meta.get("source") or "(no title)"
    stype = meta.get("source_type", "")
    text = doc.page_content or ""
    if not full and len(text) > max_chars:
        text = text[:max_chars] + "â€¦"
    print(f"[{idx}] title/source: {title}")
    print(f"     type: {stype}")
    print("----- TEXT BEGIN -----")
    print(text)
    print("------ TEXT END ------\n")

def _load_pickle(pkl_path: str) -> Any:
    if not os.path.exists(pkl_path):
        print(f"(ì—†ìŒ) {pkl_path}")
        return None
    try:
        with open(pkl_path, "rb") as f:
            return pickle.load(f)
    except Exception as e:
        print(f"{pkl_path} ë¡œë“œ ì˜¤ë¥˜: {e}")
        return None

def view_cache_pairs(pkl_path: str, start: int = 0, full: bool = False, max_chars: int = 1200) -> None:
    data = _load_pickle(pkl_path)
    if data is None:
        return
    if isinstance(data, set):
        data = list(data)
    total = len(data) if hasattr(data, "__len__") else None
    if total is None:
        print(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íƒ€ì…: {type(data)}")
        print(data)
        return
    if start >= total:
        print(f"start({start})ê°€ ì´ ê°œìˆ˜({total}) ì´ìƒì…ë‹ˆë‹¤.")
        return
    end = min(start + 2, total)
    print(f"\n=== {pkl_path} [{start} ~ {end - 1}] / ì´ {total}ê°œ ===")
    if total > 0 and isinstance(data[0], Document):
        for i in range(start, end):
            _print_doc_block(data[i], i, full=full, max_chars=max_chars)
    else:
        for i in range(start, end):
            print(f"[{i}] {data[i]}")
        print()

def view_pdf_cache_pairs(start: int = 0, full: bool = False, max_chars: int = 1200) -> None:
    view_cache_pairs(PDF_CACHE_FILE, start=start, full=full, max_chars=max_chars)

def view_web_cache_pairs(start: int = 0, full: bool = False, max_chars: int = 1200) -> None:
    view_cache_pairs("web_cache.pkl", start=start, full=full, max_chars=max_chars)

def view_faqs_cache_pairs(start: int = 0, full: bool = False, max_chars: int = 1200) -> None:
    view_cache_pairs("faqs_cache.pkl", start=start, full=full, max_chars=max_chars)

def view_processed_urls_pairs(start: int = 0, full: bool = False, max_chars: int = 1200) -> None:
    view_cache_pairs(PROCESSED_URLS_FILE, start=start, full=full, max_chars=max_chars)

# =========================
# ë²¡í„°ìŠ¤í† ì–´/ì œëª© DB ì ê²€ & ë””ë²„ê·¸
# =========================

def assert_vectorstores_ready():
    body = Chroma(persist_directory=VECTOR_STORE_DIR, embedding_function=EMBEDDINGS)
    title = Chroma(persist_directory=TITLE_VECTOR_STORE_DIR, embedding_function=EMBEDDINGS)
    try:
        body_count = len(body.get(include=[] )["ids"])
    except Exception:
        body_count = -1
    try:
        title_count = len(title.get(include=[] )["ids"])
    except Exception:
        title_count = -1
    print(f"[VS READY] body={body_count}, title={title_count}")
    if body_count <= 0 or title_count <= 0:
        print("-> ë²¡í„°ìŠ¤í† ì–´ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. build_or_update_vectorstore(force_refresh=True)ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")

def debug_find_by_title(substr: str, k: int = 10):
    title_db = Chroma(persist_directory=TITLE_VECTOR_STORE_DIR, embedding_function=EMBEDDINGS)
    q = _basic_cleanup(substr)

    def run(kv: int, use_gate: bool = True):
        hits = title_db.similarity_search_with_score(q, k=kv)
        if not hits:
            return []
        if use_gate:
            hits = [(d, dist) for d, dist in hits if _passes_lexical_gate((d.metadata or {}).get("title",""), q)]
        scored = [(d, 1 - float(dist)) for d, dist in hits]  # cosine ê°€ì •
        scored.sort(key=lambda x: _title_tiebreak_key(x[0], x[1], substr), reverse=True)
        return scored

    print(f'--- search titles for: "{substr}" ---')
    scored = run(max(k, 100), use_gate=True)
    if not scored:
        print("(gate pass=0) retry with larger k and relaxed gateâ€¦")
        scored = run(800, use_gate=False)
    if not scored:
        print("(no hits after fallback)")
        return
    for d, sim in scored[:k]:
        print(f"{sim:.3f} | {d.metadata.get('title')} | {d.metadata.get('source')}")

def _count_chroma(persist_dir: str) -> int:
    try:
        db = Chroma(persist_directory=persist_dir, embedding_function=EMBEDDINGS)
        return len(db.get(include=[])["ids"])
    except Exception:
        return 0

# =========================
# QA (RAG) â€“ ì»¤ìŠ¤í…€ 2ë‹¨ê³„ ê²€ìƒ‰ + ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸
# =========================

from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain

def query_rag(question: str):
    body_db = Chroma(persist_directory=VECTOR_STORE_DIR, embedding_function=EMBEDDINGS)
    title_db = Chroma(persist_directory=TITLE_VECTOR_STORE_DIR, embedding_function=EMBEDDINGS)

    top_docs = staged_retrieve(
        body_db, title_db, question,
        topk=8, title_k=100, body_k_per_filter=16,
        metric="cosine", variant_agg="softmax-mean", use_rrf=False
    )
    if not top_docs:
        return {"answer": "", "sources": []}

    system_text = (
        "ë‹¹ì‹ ì€ ê¸ˆìœµ ë¶„ìŸ/FAQ ë¬¸ì„œë¥¼ ê·¼ê±°ë¡œ ë‹µí•˜ëŠ” í•œêµ­ì–´ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. "
        "ë‹¤ìŒ ê·œì¹™ì„ ì§€í‚¤ì„¸ìš”:\n"
        "1) ì•„ë˜ ì œê³µë˜ëŠ” CONTEXTë§Œ ê·¼ê±°ë¡œ ë‹µë³€í•  ê²ƒ\n"
        "2) í•œêµ­ì–´ ìš©ì–´ë¥¼ ì¼ê´€ë˜ê²Œ ì‚¬ìš©í•  ê²ƒ\n"
        "3) ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë§í•  ê²ƒ\n"
        "4) ë§ˆì§€ë§‰ì— ì°¸ê³ í•œ ì¶œì²˜ ì œëª©ì„ bulletë¡œ ë‚˜ì—´í•  ê²ƒ"
    )
    human_text = (
        "ì§ˆë¬¸:\n{question}\n\n"
        "CONTEXT(ë°œì·Œ):\n{context}\n\n"
        "ìœ„ CONTEXTë§Œì„ ê·¼ê±°ë¡œ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ë‹µí•˜ì„¸ìš”."
    )
    prompt = ChatPromptTemplate.from_messages([
        SystemMessagePromptTemplate.from_template(system_text),
        HumanMessagePromptTemplate.from_template(human_text),
    ])

    stuff_chain = create_stuff_documents_chain(LLM, prompt)
    answer_text = stuff_chain.invoke({"question": question, "context": top_docs})

    # ê¹”ë”í•œ ì¶œì²˜ ëª©ë¡ êµ¬ì„±
    sources = []
    seen = set()
    for d in top_docs:
        src = (d.metadata or {}).get("source", "")
        title = (d.metadata or {}).get("title") or src or "untitled"
        if src and src not in seen:
            seen.add(src)
            sources.append({"title": title, "url": src})

    return {"answer": answer_text, "sources": sources}

# ë¬´ì‘ë‹µ/ë¶ˆí™•ì‹¤ ë‹µë³€ íŒ¨í„´ ê°ì§€
_NOANSWER_PATTERNS = (
    "ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤", "í™•ì¸ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤", "ëª¨ë¥´ê² ìŠµë‹ˆë‹¤",
    "ê·¼ê±°ê°€ ì—†ìŠµë‹ˆë‹¤", "ì œê³µëœ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ì—ì„œ", "ì¶©ë¶„í•œ ì •ë³´ê°€", "ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
)

def _is_noanswer(text: str) -> bool:
    if not text or not text.strip():
        return True
    t = text.strip()
    return any(pat in t for pat in _NOANSWER_PATTERNS)

# =========================
# Main
# =========================

if __name__ == "__main__":
    # ì œëª© ë²¡í„°ìŠ¤í† ì–´ê°€ ë¹„ì–´ ìˆìœ¼ë©´ ìë™ ë³µêµ¬ (ìºì‹œ ê¸°ë°˜)
    title_count_before = _count_chroma(TITLE_VECTOR_STORE_DIR)
    if title_count_before == 0:
        print("[INIT] Title vector store is empty â†’ building title/body stores from cachesâ€¦")
        build_or_update_vectorstore(force_refresh=False)

    print("\n" + "="*68)
    print(" ê¸ˆìœµ ë¶„ìŸ/FAQ RAG ì–´ì‹œìŠ¤í„´íŠ¸ ".center(68, " "))
    print("="*68)
    print("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”. (ì¢…ë£Œ: exit / quit / q / ì¢…ë£Œ)")
    print("-" * 68)

    while True:
        try:
            q = input("\nì§ˆë¬¸ > ").strip()
        except (EOFError, KeyboardInterrupt):
            print("\n\nì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        if not q:
            continue
        if q.lower() in ("exit", "quit", "q", "ì¢…ë£Œ"):
            print("ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        t0 = time.time()
        result = query_rag(q)
        dt = time.time() - t0

        answer = (result or {}).get("answer", "").strip()
        sources = (result or {}).get("sources", [])

        print("\n" + "="*68)
        print("ë‹µë³€".center(68, " "))
        print("-"*68)
        if _is_noanswer(answer):
            print("ì£„ì†¡í•©ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë‹µì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            show_sources = False
        else:
            print(answer)
            show_sources = True

        print("\n" + "ì¶œì²˜".center(68, " "))
        print("-"*68)
        if show_sources and sources:
            for i, s in enumerate(sources, 1):
                title = s.get("title", "untitled")
                url = s.get("url", "")
                print(f"{i}. {title}\n   â†³ {url}")
        else:
            print("í‘œì‹œí•  ì¶œì²˜ê°€ ì—†ìŠµë‹ˆë‹¤.")

        print("\nì²˜ë¦¬ì‹œê°„: {:.2f}s".format(dt))
        print("="*68)
